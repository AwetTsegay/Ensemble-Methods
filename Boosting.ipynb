{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg(data,a):\n",
    "    if len(a) == 0:\n",
    "        return 0.0\n",
    "    ttl = 0.0\n",
    "    for aa in a:\n",
    "        ttl += data[aa][-1]\n",
    "        \n",
    "        return ttl / len(a)\n",
    "    \n",
    "data = [(1,6,6) , (2,4,2) , (3,7,10) , (5,10,20) , (7,12,18) , (8,6,12)]\n",
    "a = (1, 2)\n",
    "avg(data, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1,6,6) , (5,4,7)]\n",
    "data[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1,6,6) , (5,4,7)]\n",
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1,6) , (5,4)]\n",
    "data[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = (1, 2)\n",
    "for aa in a:\n",
    "    print (aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-23-d5c681191961>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-d5c681191961>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    return (data[aa][-1])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "data = [(1,6,6) , (5,4,7)]\n",
    "a = (0, 1)\n",
    "for aa in a:\n",
    "    return (data[aa][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rss(data,a):\n",
    "    if len(a) == 0:\n",
    "        return 0.0\n",
    "    mean = avg(data,a)\n",
    "    ttl = 0.0\n",
    "    for aa in a:\n",
    "        ttl += ((data[aa][-1] - mean) * (data[aa][-1] - mean))\n",
    "    return ttl\n",
    "\n",
    "data = [(1,2,3) , (2,4,1)]\n",
    "a = (0, 1)\n",
    "rss(data, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionTree:\n",
    "    #There are two types of nodes.\n",
    "    #When the node is a leaf, then self.leaf = True, and the prediction is the average of the labels of the data reaching this leaf.\n",
    "    #When the node is not a leaf, then self.attr and self.split record the optimal split, and self.L, self.R are two sub-decision-trees.\n",
    "   \n",
    "    def __init__(self,data,indx,depth): #if you do not want to limit depth, you can set depth = len(data)\n",
    "        \n",
    "        if depth==0: #if depth = 0, that means we don't go further down the tree\n",
    "            self.leaf = True #so it is a leaf\n",
    "            self.prediction = avg(data,indx) #and the prediction is the average of all labels in data[indx]\n",
    "            \n",
    "        elif len(set([data[i][-1] for i in indx])) == 1:  #when all labels in data[indx] are the same, we don't need to go further down the tree\n",
    "            self.leaf = True #this includes the case when len(indx)==1; so it is a leaf\n",
    "            self.prediction = data[indx[0]][-1] #and the prediction is simply the common label value\n",
    "            \n",
    "        else: \n",
    "            #otherwise, we need to do splitting; computing optimal split below\n",
    "            self.leaf = False #so it is not a leaf\n",
    "            self.attr , self.split , self.L , self.R = self.generate(data,indx,depth)\n",
    "            #attr stores which attribute is used to split\n",
    "            #split stores the numerical value used to split into left and right subtrees\n",
    "            #L and R are left and right subtrees\n",
    "    \n",
    "    def generate(self,data,indx,depth): #generate the decision tree down-wards\n",
    "        \n",
    "        p = len(data[indx[0]])-1\n",
    "        opt = 1000000000000.0\n",
    "        for j in range(p): \n",
    "            #for each attribute, we search the optimal split\n",
    "            all_cuts = set( [ data[i][j] for i in indx ] ) #we find out all possible split values of the attribute we are considering\n",
    "            \n",
    "            for cut in all_cuts:\n",
    "                yl = [ i for i in indx if data[i][j]<=cut]\n",
    "                yr = [ i for i in indx if data[i][j]>cut]\n",
    "                tmp = rss(data,yl) + rss(data,yr)\n",
    "                if tmp < opt:\n",
    "                    opt , attr , split, L , R = tmp , j , cut , yl , yr\n",
    "        return attr , split , decisionTree(data,L,depth-1) , decisionTree(data,R,depth-1)\n",
    "\n",
    "    def predict(self,x):\n",
    "        if self.leaf == True:\n",
    "            return self.prediction\n",
    "        if (x[self.attr] <= self.split):\n",
    "            return self.L.predict(x)\n",
    "        return self.R.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the learning rate. 0.9\n",
      "[(1, 6, 6), (2, 4, 2), (3, 7, 10), (5, 10, 20), (7, 12, 18), (8, 6, 12)]\n",
      "---------------------------------------------------------------------------\n",
      "Iteration 0: [6.0, 2.0, 10.0, 20.0, 18.0, 12.0]\n"
     ]
    }
   ],
   "source": [
    "eta = input(\"Enter the learning rate. \")\n",
    "eta = float(eta)\n",
    "data = [(1,6,6) , (2,4,2) , (3,7,10) , (5,10,20) , (7,12,18) , (8,6,12)]\n",
    "#this is the small dataset we used in the lecture as an example\n",
    "print(data)\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Iteration \" + str(0) + \": \" + str([round(t[-1]*10000)/10000 for t in data ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: [4.2, 0.2, 8.2, 2.0, 1.8, 1.2]\n",
      "Iteration 2: [0.42, 0.11, 0.82, 1.1, 0.9, 1.11]\n",
      "Iteration 3: [0.231, -0.079, 0.082, 0.605, 0.405, 0.111]\n",
      "Iteration 4: [0.1617, -0.0079, 0.0127, 0.0605, 0.0405, 0.0417]\n",
      "Iteration 5: [0.0162, -0.0043, 0.0163, 0.0424, 0.0224, 0.0235]\n",
      "Iteration 6: [0.0113, -0.0092, 0.0114, 0.0042, 0.0022, 0.0024]\n",
      "Iteration 7: [0.0062, -0.0009, 0.0063, 0.003, 0.001, 0.0011]\n",
      "Iteration 8: [0.0006, -0.0005, 0.0006, 0.0016, -0.0004, 0.0015]\n",
      "Iteration 9: [0.0005, -0.0001, 0.0005, 0.0015, -0.0005, 0.0001]\n",
      "Iteration 10: [0.0004, -0.0002, 0.0004, 0.0001, -0.0001, 0.0]\n",
      "Iteration 11: [0.0002, 0.0, 0.0002, 0.0001, -0.0001, 0.0]\n",
      "Iteration 12: [0.0001, -0.0001, 0.0001, 0.0, 0.0, 0.0]\n",
      "Iteration 13: [0.0001, 0.0, 0.0001, 0.0, 0.0, 0.0]\n",
      "Iteration 14: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 15: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 16: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 17: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 18: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 19: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 20: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 21: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 22: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 23: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 24: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 25: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 26: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 27: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 28: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 29: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 30: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 31: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 32: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 33: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 34: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 35: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 36: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 37: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 38: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 39: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 40: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 41: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 42: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 43: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 44: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 45: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 46: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 47: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 48: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 49: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 50: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 51: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 52: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 53: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 54: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 55: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 56: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 57: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 58: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 59: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 60: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 61: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 62: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 63: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 64: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 65: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 66: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 67: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 68: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 69: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 70: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 71: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 72: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 73: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 74: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 75: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 76: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 77: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 78: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 79: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 80: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 81: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 82: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 83: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 84: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 85: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 86: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 87: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 88: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 89: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 90: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 91: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 92: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 93: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 94: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 95: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 96: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 97: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 98: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 99: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Iteration 100: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#all_data = [ data ]\n",
    "residue = data\n",
    "hatf = [] #this is used to store the decision trees generated\n",
    "for i in range(100):\n",
    "    hatf.append(decisionTree(residue, list(range(len(residue))), 2 ))\n",
    "    #all_data.append ( [ (t[0] , t[1] , t[2] - eta * hatf[-1].predict( (t[0],t[1]) ) ) for t in all_data[-1] ] )\n",
    "    residue = [ t[:-1] + ( t[-1] - eta * hatf[-1].predict( t[:-1] ) , ) for t in residue ]\n",
    "    \n",
    "    print(\"Iteration \" + str(i+1) + \": \" + str([ round(t[-1]*10000)/10000 for t in residue ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
